{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_[insert in-house element tl:dr] This article provides a brief introduction to natural language using [spaCy](https://spacy.io/) and related libraries in Python. The complementary Domino project is also available [insert link]._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: An Introduction to Natural Language in Python using spaCy\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial provides a brief introduction to working with natural language (sometimes called \"text analytics\") in Python, using [spaCy](https://spacy.io/) and related libraries.\n",
    "Data science teams in industry must work with lots of text, which is one of the top four categories of data used in machine learning.\n",
    "Usually that's human-generated text, although not always.\n",
    "\n",
    "Think about it: how does the \"operating system\" for business work? Typically, there are contracts (sales contracts, work agreements, partnerships), there are invoices, there are insurance policies, there are regulations and other laws, and so one.\n",
    "All of those are represented as text.\n",
    "When machines parse text, that's generally called _natural language processing_ (NLP).\n",
    "When machines learn to interpret what's in the text, that's generally called _natural language understanding_ (NLU).\n",
    "When machines in turn create text, that's generally called _natural language generation_ (NLG).\n",
    "\n",
    "The _spaCy_ framework -- along with a wide and growing range of plug-ins and other integrations -- provides features for NLP, the groundwork for NLU, and even some aspects of supporting NLG.\n",
    "It's become one of the most widely used natural language libraries in Python for industry use cases, and has quite a large community -- and with that, much support for commercialization of research advances as this area continues to evolve rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Check out the _spaCy_ [installation notions](https://spacy.io/usage) for a \"configurator\" based on which platforms and natural languages you need to support.\n",
    "Those notes describe how to use `pip` for installation, but save yourself large amounts of grief and simply use `conda` instead.\n",
    "Since about version 2.x, _spaCy_ has become more difficult to install correctly via `pip` even on vanilla instances of Linux.\n",
    "That's a [criticism](https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/) about Python's use of of the relatively fragile [wheels](https://www.python.org/dev/peps/pep-0427/) for package distribution, not a critique of _spaCy_ -- much the same caveats would apply to many other frameworks written in Python.\n",
    "Use `conda` and save yourself much grief.\n",
    "\n",
    "To get started with _spaCy_ working with text in English on a Linux system:\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "BTW, the second line above is a download for language resources (models, etc.) and the `_sm` at the end of the download's name indicates a \"small\" model. There's also \"medium\" and \"large\", albeit those are quite large. Some of the more advanced features depend on the later, although we won't quite be diving to the bottom of that ocean in this (brief) tutorial.\n",
    "\n",
    "Now let's load _spaCy_ and run some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That `nlp` variable now is your gateway to all things _spaCy_ and loaded with the `en_core_web_sm` small model for English.\n",
    "Next, let's run a small \"document\" through the natural language parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET True\n",
      "rain rain NOUN False\n",
      "in in ADP True\n",
      "Spain Spain PROPN False\n",
      "falls fall VERB False\n",
      "mainly mainly ADV False\n",
      "on on ADP True\n",
      "the the DET True\n",
      "plain plain NOUN False\n",
      ". . PUNCT False\n"
     ]
    }
   ],
   "source": [
    "text = u\"The rain in Spain falls mainly on the plain.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we created a [doc](https://spacy.io/api/doc) from the text, which is a container for a document and all of its annotations. Then we iterated through the document, to see what _spaCy_ had parsed.\n",
    "\n",
    "Good, but it's a lot of info, and a bit difficult to read. Let's reformat the _spaCy_ parse of that sentence as a [pandas](https://pandas.pydata.org/) dataframe, using a [_list comprehension_](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rain</td>\n",
       "      <td>rain</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>falls</td>\n",
       "      <td>fall</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mainly</td>\n",
       "      <td>mainly</td>\n",
       "      <td>ADV</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text   lemma    POS  stopword\n",
       "0     The     the    DET      True\n",
       "1    rain    rain   NOUN     False\n",
       "2      in      in    ADP      True\n",
       "3   Spain   Spain  PROPN     False\n",
       "4   falls    fall   VERB     False\n",
       "5  mainly  mainly    ADV     False\n",
       "6      on      on    ADP      True\n",
       "7     the     the    DET      True\n",
       "8   plain   plain   NOUN     False\n",
       "9       .       .  PUNCT     False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = [ [token.text, token.lemma_, token.pos_, token.is_stop] for token in doc ]\n",
    "df = pd.DataFrame(rows, columns=(\"text\", \"lemma\", \"POS\", \"stopword\"))\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more readable!\n",
    "In this simple case, the entire document is merely one short sentence.\n",
    "For each word in that sentence, _spaCy_ has created a [token](https://spacy.io/api/token), and we accessed fields in each token to show:\n",
    "\n",
    " - raw text\n",
    " - [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)) – a root form of the word\n",
    " - [part of speech](https://en.wikipedia.org/wiki/Part_of_speech)\n",
    " - a flag for whether the word is a _stopword_ – i.e., a common word that may be filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's use the [displaCy](https://ines.io/blog/developing-displacy) library to visualize the parse tree for that sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a19d1b60b0ad4d798caf47cf35afdeee-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">rain</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Spain</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">falls</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mainly</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">plain.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-7\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a19d1b60b0ad4d798caf47cf35afdeee-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,266.5 L1453.0,254.5 1437.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does that bring back memories of grade school? Frankly, for those of us coming from more of a computational linguistics background, that diagram sparks joy.\n",
    "\n",
    "But let's backup for a moment. How do you handle multiple sentences?\n",
    "\n",
    "There are features for _sentence boundary detection_ (SBD) – also known as _sentence segmentation_ – based on the builtin/default [sentencizer](https://spacy.io/api/sentencizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.\n",
      "> I fell in.\n",
      "> Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket.\n",
      "> The gorillas just went wild.\n"
     ]
    }
   ],
   "source": [
    "text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in. Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When _spaCy_ creates a document, it uses a principle of _non-destructive tokenization_ such that the tokens, sentences, etc., are simply indexes into a long array. In other words, they don't carve the text stream into little pieces. So each sentence is a [span](https://spacy.io/api/span) with a _start_ and an _end_ index into the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0 25\n",
      "> 25 29\n",
      "> 29 48\n",
      "> 48 54\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\">\", sent.start, sent.end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index into the document to pull out the tokens for one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The gorillas just went wild."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[48:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply index into a specific token, such as the verb `went` in the last sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went go VERB\n"
     ]
    }
   ],
   "source": [
    "token = doc[51]\n",
    "print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can parse a document, segment that document into sentences, then look at annotations about the tokens in each sentence. That's a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring Text\n",
    "\n",
    "Now that we can parse texts, where do we get texts?\n",
    "One quick source is to leverage the interwebs.\n",
    "Of course when we download web pages we'll get HTML, and then need to extract text from them.\n",
    "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a popular package for that.\n",
    "\n",
    "First, a little housekeeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function `get_text()` we'll parse the HTML to find all of the `<p/>` tags, then extract the text for those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "def get_text (url):\n",
    "    buf = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "        \n",
    "        for p in soup.find_all(\"p\"):\n",
    "            buf.append(p.get_text())\n",
    "\n",
    "        return \"\\n\".join(buf)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab some text from online sources.\n",
    "We can compare open source licenses hosted on the [Open Source Initiative](https://opensource.org/licenses/) site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SPDX short identifier: BSD-3-Clause\n",
      " \n",
      "\n",
      "\n",
      "> Note: This license has also been called the \"New BSD License\" or \"Modified BSD License\".\n",
      "> See also the 2-clause BSD License.\n",
      "\n",
      "> Copyright <YEAR>\n",
      "> <\n",
      "> COPYRIGHT HOLDER>\n",
      "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
      "1.\n",
      "> Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
      "\n",
      "> 2.\n",
      "> Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
      "\n",
      "> 3.\n",
      "> Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
      "\n",
      "> THIS SOFTWARE IS\n",
      "> PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
      "> \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,\n",
      "> BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
      "> ARE DISCLAIMED.\n",
      "> IN NO EVENT SHALL\n",
      "> THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n",
      "> BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n",
      "> HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n",
      "> OTHERWISE)\n",
      "> ARISING IN ANY WAY\n",
      "> OUT OF THE USE OF THIS SOFTWARE,\n",
      "> EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "   \n",
      " \n",
      "\n",
      "> The content on this website, of which Opensource.org is the author, is licensed under a Creative Commons Attribution 4.0 International License.\n",
      "\n",
      "> Opensource.org is not the author of any of the licenses reproduced on this site.\n",
      "> Questions about the copyright in a license should be directed to the license steward.\n",
      "\n",
      "> Hosting for Opensource.org is generously provided by DigitalOcean.\n",
      "> Please see Terms of Service.\n",
      "> For questions regarding the OSI website and contents pleasee email our webmaster.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "lic = {}\n",
    "lic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))\n",
    "lic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-2.0\"))\n",
    "lic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))\n",
    "\n",
    "for sent in lic[\"bsd\"].sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common use case for natural language work is to compare texts. For example, with these open source licenses, we can download their text, parse, then compare [similarity](https://spacy.io/api/doc#similarity) metrics among them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mit asl 0.9482039305669306\n",
      "asl bsd 0.9391555350757145\n",
      "bsd mit 0.9895838089575453\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    [\"mit\", \"asl\"],\n",
    "    [\"asl\", \"bsd\"],\n",
    "    [\"bsd\", \"mit\"]\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    print(a, b, lic[a].similarity(lic[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting, since the [BSD](https://opensource.org/licenses/BSD-3-Clause) and [MIT](https://opensource.org/licenses/MIT) licenses appear to be the most similar documents.\n",
    "In fact they are closely related.\n",
    "\n",
    "Admittedly, there was some extra text included in each document due to the OSI disclaimer in the footer – but this provides a reasonable approximation for comparing the licenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Understanding\n",
    "\n",
    "Now let's dive into some of the _spaCy_ features for NLU.\n",
    "Given that we have a parse of a document, from a purely grammatical standpoint we can pull the [noun chunks](https://spacy.io/usage/linguistic-features#noun-chunks), i.e., each of the noun phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs\n",
      "Steve Wozniak\n",
      "Apple Computer\n",
      "January\n",
      "Cupertino\n",
      "California\n"
     ]
    }
   ],
   "source": [
    "text = u\"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. The noun phrases in a sentence generally provide more information content – as a simple filter used to reduce a long document into a more \"distilled\" representation.\n",
    "\n",
    "We can take this approach further, and identify [named entities](https://spacy.io/usage/linguistic-features#named-entities) within the text, i.e., the proper nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs PERSON\n",
      "Steve Wozniak PERSON\n",
      "Apple Computer ORG\n",
      "January 3, 1977 DATE\n",
      "Cupertino GPE\n",
      "California GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _displaCy_ library provides an excellent way to visualize named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " incorporated \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple Computer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    January 3, 1977\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Cupertino\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're working with [knowledge graph](https://www.akbc.ws/) applications and other [linked data](http://linkeddata.org/), the challenge is to construct links between the named entities in a document and other related information for the entities – which is called [entity linking](http://nlpprogress.com/english/entity_linking.html).  Identifying the named entities in a document is the first step in that particular kind of AI work.\n",
    "For example, given text above one might link the `Steve Wozniak` named entity to a [lookup in DBpedia](http://dbpedia.org/page/Steve_Wozniak)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more general terms, one can also link _lemmas_ to resources that describe their meanings.\n",
    "For example, in an early section we parsed the sentence `The gorillas just went wild` and were able to show that the lemma for the word `went` is the verb `go`. At that point we can use a venerable project called [WordNet](https://wordnet.princeton.edu/) which provides a lexical database for English – in other words, it's a computable thesaurus.\n",
    "\n",
    "There's a _spaCy_ integration for WordNet called\n",
    "[spacy-wordnet](https://github.com/recognai/spacy-wordnet) by [Daniel Vila Suero](https://twitter.com/dvilasuero), an expert in natural language and knowledge graph work.\n",
    "To install, use `pip` under `conda` to load directly from the GitHub project:\n",
    "```\n",
    "pip install spacy-wordnet\n",
    "```\n",
    "\n",
    "Then we'll load the WordNet data via NLTK (these things happen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ceteri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that _spaCy_ runs as a \"pipeline\" and allows means for customizing parts of the pipeline in use.\n",
    "That's excellent for supporting really interesting workflow integrations in data science work.\n",
    "Here we'll add the `WordnetAnnotator` from the _spacy-wordnet_ project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ['tagger', 'parser', 'ner']\n",
      "after ['tagger', 'WordnetAnnotator', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "\n",
    "print(\"before\", nlp.pipe_names)\n",
    "\n",
    "if \"WordnetAnnotator\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n",
    "    \n",
    "print(\"after\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the English language, some words are infamous for having many possible meanings. For example, click through the results online in a [WordNet](http://wordnetweb.princeton.edu/perl/webwn?s=star&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=) search to find the meanings related to the word `withdraw`.\n",
    "\n",
    "Now let's use _spaCy_ to perform that lookup automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('withdraw.v.01'),\n",
       " Synset('retire.v.02'),\n",
       " Synset('disengage.v.01'),\n",
       " Synset('recall.v.07'),\n",
       " Synset('swallow.v.05'),\n",
       " Synset('seclude.v.01'),\n",
       " Synset('adjourn.v.02'),\n",
       " Synset('bow_out.v.02'),\n",
       " Synset('withdraw.v.09'),\n",
       " Synset('retire.v.08'),\n",
       " Synset('retreat.v.04'),\n",
       " Synset('remove.v.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = nlp(\"withdraw\")[0]\n",
    "token._.wordnet.synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('withdraw.v.01.withdraw'),\n",
       " Lemma('withdraw.v.01.retreat'),\n",
       " Lemma('withdraw.v.01.pull_away'),\n",
       " Lemma('withdraw.v.01.draw_back'),\n",
       " Lemma('withdraw.v.01.recede'),\n",
       " Lemma('withdraw.v.01.pull_back'),\n",
       " Lemma('withdraw.v.01.retire'),\n",
       " Lemma('withdraw.v.01.move_back'),\n",
       " Lemma('retire.v.02.retire'),\n",
       " Lemma('retire.v.02.withdraw'),\n",
       " Lemma('disengage.v.01.disengage'),\n",
       " Lemma('disengage.v.01.withdraw'),\n",
       " Lemma('recall.v.07.recall'),\n",
       " Lemma('recall.v.07.call_in'),\n",
       " Lemma('recall.v.07.call_back'),\n",
       " Lemma('recall.v.07.withdraw'),\n",
       " Lemma('swallow.v.05.swallow'),\n",
       " Lemma('swallow.v.05.take_back'),\n",
       " Lemma('swallow.v.05.unsay'),\n",
       " Lemma('swallow.v.05.withdraw'),\n",
       " Lemma('seclude.v.01.seclude'),\n",
       " Lemma('seclude.v.01.sequester'),\n",
       " Lemma('seclude.v.01.sequestrate'),\n",
       " Lemma('seclude.v.01.withdraw'),\n",
       " Lemma('adjourn.v.02.adjourn'),\n",
       " Lemma('adjourn.v.02.withdraw'),\n",
       " Lemma('adjourn.v.02.retire'),\n",
       " Lemma('bow_out.v.02.bow_out'),\n",
       " Lemma('bow_out.v.02.withdraw'),\n",
       " Lemma('withdraw.v.09.withdraw'),\n",
       " Lemma('withdraw.v.09.draw'),\n",
       " Lemma('withdraw.v.09.take_out'),\n",
       " Lemma('withdraw.v.09.draw_off'),\n",
       " Lemma('retire.v.08.retire'),\n",
       " Lemma('retire.v.08.withdraw'),\n",
       " Lemma('retreat.v.04.retreat'),\n",
       " Lemma('retreat.v.04.pull_back'),\n",
       " Lemma('retreat.v.04.back_out'),\n",
       " Lemma('retreat.v.04.back_away'),\n",
       " Lemma('retreat.v.04.crawfish'),\n",
       " Lemma('retreat.v.04.crawfish_out'),\n",
       " Lemma('retreat.v.04.pull_in_one's_horns'),\n",
       " Lemma('retreat.v.04.withdraw'),\n",
       " Lemma('remove.v.01.remove'),\n",
       " Lemma('remove.v.01.take'),\n",
       " Lemma('remove.v.01.take_away'),\n",
       " Lemma('remove.v.01.withdraw')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.wordnet.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astronomy',\n",
       " 'school',\n",
       " 'telegraphy',\n",
       " 'industry',\n",
       " 'psychology',\n",
       " 'ethnology',\n",
       " 'ethnology',\n",
       " 'administration',\n",
       " 'school',\n",
       " 'finance',\n",
       " 'economy',\n",
       " 'exchange',\n",
       " 'banking',\n",
       " 'commerce',\n",
       " 'medicine',\n",
       " 'ethnology',\n",
       " 'university',\n",
       " 'school',\n",
       " 'buildings',\n",
       " 'factotum',\n",
       " 'agriculture',\n",
       " 'mechanics',\n",
       " 'gastronomy',\n",
       " 'meteorology',\n",
       " 'physics',\n",
       " 'basketball',\n",
       " 'anatomy',\n",
       " 'skiing',\n",
       " 'nautical',\n",
       " 'engineering',\n",
       " 'racing',\n",
       " 'home',\n",
       " 'drawing',\n",
       " 'dentistry',\n",
       " 'ethnology',\n",
       " 'mathematics',\n",
       " 'furniture',\n",
       " 'animal_husbandry',\n",
       " 'industry',\n",
       " 'economy',\n",
       " 'body_care',\n",
       " 'chemistry',\n",
       " 'medicine',\n",
       " 'surgery',\n",
       " 'vehicles',\n",
       " 'transport',\n",
       " 'atomic_physic',\n",
       " 'archaeology',\n",
       " 'hydraulics',\n",
       " 'oceanography',\n",
       " 'golf',\n",
       " 'sculpture',\n",
       " 'earth',\n",
       " 'applied_science',\n",
       " 'artisanship']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.wordnet.wordnet_domains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if you were working with knowledge graphs, those \"word sense\" links from WordNet could be used along with graph algorithms to help identify the meanings for a particular word. That can also be used to develop summaries for larger sections of text, in a technique that's called _summarization_.  It's beyond the scope of this tutorial, but an interesting application currently for natural language in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going the other direction, if you know _a priori_ that a document was about a particular domain or set of topics, then you can constrain the meanings returned from WordNet. In the following example, we want to consider NLU results that are within Finance and Banking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I (require|want|need) to (draw_off|withdraw|draw|take_out) 5,000 euros .\n"
     ]
    }
   ],
   "source": [
    "domains = [\"finance\", \"banking\"]\n",
    "sentence = nlp(u\"I want to withdraw 5,000 euros.\")\n",
    "\n",
    "enriched_sent = []\n",
    "\n",
    "for token in sentence:\n",
    "    # get synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
    "    \n",
    "    if synsets:\n",
    "        lemmas_for_synset = []\n",
    "        \n",
    "        for s in synsets:\n",
    "            # get synset variants and add to the enriched sentence\n",
    "            lemmas_for_synset.extend(s.lemma_names())\n",
    "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
    "    else:\n",
    "        enriched_sent.append(token.text)\n",
    "\n",
    "print(\" \".join(enriched_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That example may look simple, although if you play with the `domains` list you'll find that the results have a kind of combinatorial explosion when run without reasonable constraints. Imagine having a knowledge graph with millions of elements: you'd want to constrain searches where possible to avoid having every query take days/weeks/months/years to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the problems encountered when trying to understand a text – or better yet when trying to understand a _corpus_ (a dataset with many related texts) – become so complex that you need to visualize first. Here's an interactive visualization for understanding texts: [scattertext](https://spacy.io/universe/project/scattertext), a product of the genius of [Jason Kessler](https://twitter.com/jasonkessler).\n",
    "To install:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge scattertext\n",
    "```\n",
    "\n",
    "Then let's analyze text data from the party conventions during the 2012 US Presidential elections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "\n",
    "convention_df = st.SampleCorpora.ConventionData2012.get_data() \n",
    "corpus = st.CorpusFromPandas(convention_df,\n",
    "                             category_col=\"party\",\n",
    "                             text_col='text',\n",
    "                             nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, that might take a few minutes to compute.  Once you have the `corpus` ready, then generate an interactive visualization in HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category=\"democrat\",\n",
    "    category_name=\"Democratic\",\n",
    "    not_category_name=\"Republican\",\n",
    "    width_in_pixels=1000,\n",
    "    metadata=convention_df[\"speaker\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll render the HTML – give it a minute or two to load, it's worth the wait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"foo.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7efe6867e908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "file_name = \"foo.html\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine if you had text from the past three years of customer support for a particular product in your organization. Suppose your team needed to understand how customers had been talking about the product? This _scattertext_ library might come in quite handy! You could cluster (k=2) on _NPS scores_ (a customer evaluation metric) then replace the Democrat/Republican dimension with the top two components from the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Five years ago, if you’d asked about open source in Python for natural language, a default answer from many people working in data science would've been [NLTK](https://www.nltk.org/).\n",
    "That project includes just about everything but the kitchen sink and has components which are relatively academic.\n",
    "Another popular natural language project is [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) from Stanford.\n",
    "Also quite academic, albeit powerful -- though _CoreNLP_ can be challenging to integrate with other software for production use.\n",
    "\n",
    "Then a few years ago everything in this corner of the world began to change.\n",
    "The two principal authors for _spaCy_ -- [Matthew Honnibal](https://twitter.com/honnibal) and [Ines Montani](https://twitter.com/_inesmontani) -- launched the project in 2015, and industry adoption was rapid.\n",
    "They focused on an _opinionated_ approach (do what's needed, do it well, no more, no less) which provided simple, rapid integration into data science workflows in Python, as well as faster execution and better accuracy than the alternatives.\n",
    "Based on those priorities, _spaCy_ become sort of the opposite of _NLTK_.\n",
    "Since 2015, _spaCy_ has consistently focused on being an open source project (i.e., depending on its community for directions, integrations, etc.) and being commercial-grade software (not academic research).\n",
    "That said, _spaCy_ has been quick to incorporate the SOTA advances in machine learning, effectively becoming a conduit for moving research into industry.\n",
    "\n",
    "It's important to note that machine learning for natural language got a big boost during the mid-2000's as Google began to win international language translation competitions.\n",
    "Another big change occurred during 2017-2018 when, following the many successes of _deep learning_, those approaches began to out-perform previous machine learning models.\n",
    "For example, see the [ELMo](https://arxiv.org/abs/1802.05365) work on _language embedding_ by Allen AI, followed by [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) from Google, and more recently [ERNIE](https://medium.com/syncedreview/baidus-ernie-tops-google-s-bert-in-chinese-nlp-tasks-d6a42b49223d) by Baidu -- in other words, the search engine giants of the world have gifted the rest of us with a Sesame Street repertoire of open source embedded language models based on deep learning, which is now _state of the art_ (SOTA).\n",
    "Speaking of which, to keep track of SOTA for natural language, keep an eye on [NLP-Progress](http://nlpprogress.com/) and [Papers with Code](https://paperswithcode.com/sota).\n",
    "\n",
    "Meanwhile the use cases for natural language shifted dramatically over the past two years, after deep learning techniques arose to the fore.\n",
    "Circa 2014, a natural language tutorial in Python might have shown _word count_ or _keyword search_ or _sentiment detection_ where the target use cases were relatively underwhelming.\n",
    "Circa 2019 we're talking about analyzing thousands of documents for vendor contracts in an industrial supply chain optimization ... or hundreds of millions of documents for policy holders of an insurance company, or gazillions of documents regarding financial disclosures. More contemporary natural language work tends to be in NLU, often to support construction of _knowledge graphs,_ and increasingly in NLG where for example large numbers of similar documents can be summarized at human scale.\n",
    "\n",
    "The [spaCy Universe](https://spacy.io/universe) is a great place to check for deep-dives into particular use cases, and to see how this field is evolving. Some selections from that \"universe\" include:\n",
    "\n",
    " - [Blackstone](https://spacy.io/universe/project/blackstone) – parsing unstructured legal texts\n",
    " - [Kindred](https://spacy.io/universe/project/kindred) – extracting entities from biomedical texts (e.g., Pharma)\n",
    " - [mordecai](https://spacy.io/universe/project/mordecai) – parsing geographic information\n",
    " - [Prodigy](https://spacy.io/universe/project/prodigy) – human-in-the-loop annotation to label datasets\n",
    " - [spacy-raspberry](https://spacy.io/universe/project/spacy-raspberry) – Raspberry PI image for running _spaCy_ and deep learning on edge devices\n",
    " - [Rasa NLU](https://spacy.io/universe/project/rasa) – Rasa integration for voice apps\n",
    " \n",
    "There's much more we could have done with _spaCy_ – although hopefully this tutorial provides an introduction. We wish you all the best in your natural language work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
